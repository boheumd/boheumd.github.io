
<!DOCTYPE html>
<html>
<style>

p {
  font-size: 15px;
}

</style>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>D-NeRV</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.">


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main" >
        <div class="row">
            <h2 class="col-md-12 text-center">
                Towards Scalable Neural Representation for Diverse Videos</br> 
                <small>
                    CVPR 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                      <a href="https://boheumd.github.io/"><strong>Bo He<sup>1</sup></strong></a>,
                      <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang<sup>2</sup></a>,
                      <a href="https://hywang66.github.io/">Hanyu Wang<sup>1</sup></a>,
                      <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu<sup>3</sup></a>,
                      <a href="https://haochen-rye.github.io/">Hao Chen<sup>1</sup></a>,
                      <a href="https://shuaiyihuang.github.io/">Shuaiyi Huang<sup>1</sup></a>,
                      <br>
                      <a href="https://scholar.google.com/citations?user=TFpdak8AAAAJ&hl=en">Yixuan Ren<sup>1</sup></a>,
                      <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim<sup>2</sup></a>,
                      <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava<sup>1</sup></a>
                    </li>
                </ul>
                <sup>1</sup> University of Maryland, College Park &emsp; <sup>2</sup> Meta AI &emsp; <sup>3</sup> Fudan University
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2303.14124">
                               <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/boheumd/D-NeRV">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Implicit neural representations (INR) have gained increasing attention in representing 3D scenes and images, and have been recently applied to encode videos (e.g., NeRV, E-NeRV).
                    While achieving promising results, existing INR-based methods are limited to encoding a handful of short videos (e.g., seven 5-second videos in the UVG dataset) with redundant visual content, leading to a model design that fits individual video frames independently and is not efficiently scalable to a large number of diverse videos.
                    This paper focuses on developing neural representations for a more practical setup -- encoding long and/or a large number of videos with diverse visual content.
                    We first show that instead of dividing videos into small subsets and encoding them with separate models, encoding long and diverse videos jointly with a unified model achieves better compression results. Based on this observation, we propose D-NeRV, a novel neural representation framework designed to encode diverse videos by (i) decoupling clip-specific visual content from motion information, (ii) introducing temporal reasoning into the implicit neural network, and (iii) employing the task-oriented flow as intermediate output to reduce spatial redundancies.
                    Our new model largely surpasses NeRV and traditional video compression techniques on UCF101 and UVG datasets on the video compression task. Moreover, when used as an efficient data-loader, D-NeRV achieves 3%-10% higher accuracy than NeRV on action recognition tasks on the UCF101 dataset under the same compression ratios.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>NeRV vs. D-NeRV</h3>
                <p class="text-justify">
                    Comparison of D-NeRV and NeRV when representing diverse videos. NeRV optimizes representation to every video independently while D-NeRV encodes all videos by a shared model and contioned by keyframes from each video.
                </p>
                <p style="text-align:center;">
                    <image src="images/teaser.png" height="180px" >
                </p>
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                    NeRV &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                    D-NeRV
                <br>

                <h3>Model overview</h3>
                <p class="text-justify">
                    D-NeRV takes in key-frame pairs of each video clip along with all the frame indices and outputs a whole video clip at a time. The decoder block predicts the flow estimation to warp the visual content feature from the encoder, then fuses the visual content by the spatially-adaptive fusion module and finally models temporal relationship by the global temporal MLP module.
                    <ul>
                    <li>The Visual Content Encoder captures clip-specific visual content.</li>
                    <li>The Flow Preidction module estimates task-oriented flow to reduce spatial redundacies across frames.</li>
                    <li>The Spatially-adaptive Fusion (SAF) module utilizes the content feature map as a modulation for decoder features.</li>
                    <li>The Global Temporal MLP (GTMLP) module exploit explicit temporal relationship of videos.</li>
                    </ul>
                </p>
                <p style="text-align:center;">
                    <image src="images/model.png" height="240px" >
                </p>


                <h3>Video Compression</h3>
                <h4 align="center">
                    UVG Dataset
                </h4>
                <div class="row">
                    <div class="column">
                    <center>
                    <img src="images/uvg/uvg_psnr_bpp.png" alt="uvg_psnr" style="width:48%">
                    <img src="images/uvg/uvg_ssim_bpp.png" alt="uvg_ssim" style="width:48%">
                    </center>
                    </div>
                </div>
                <br> 

                <h4 align="center">
                    UCF-101 Dataset
                </h4>
                <div class="row">
                  <div class="column">
                    <center>
                    <img src="images/ucf101/gt_v_WritingOnBoard_g17_c05.gif" alt="gt" style="width:24%">
                    <img src="images/ucf101/h264_v_WritingOnBoard_g17_c05.gif" alt="h264" style="width:24%">
                    <img src="images/ucf101/nerv_v_WritingOnBoard_g17_c05.gif" alt="NeRV" style="width:24%">
                    <img src="images/ucf101/dnerv_v_WritingOnBoard_g17_c05.gif" alt="D-NeRV" style="width:24%">
                    </center>
                  </div>
                  <div class="column">
                    <center>
                    <img src="images/ucf101/gt_v_PushUps_g16_c04.gif" alt="gt" style="width:24%">
                    <img src="images/ucf101/h264_v_PushUps_g16_c04.gif" alt="h264" style="width:24%">
                    <img src="images/ucf101/nerv_v_PushUps_g16_c04.gif" alt="NeRV" style="width:24%">
                    <img src="images/ucf101/dnerv_v_PushUps_g16_c04.gif" alt="D-NeRV" style="width:24%">
                    </center>
                  </div>
                     &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     GT &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     H264 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     NeRV &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     D-NeRV &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
                </div>
                <h5 align="center">Comparision under the same compression ratio</h5>
                <br>

                <h3>Action Recongnition</h3>
                <h4 align="center">
                    UCF-101 Dataset
                </h4>
                <p style="text-align:center;">
                    <image src="images/action_recognition.png" height="200px" >
                </p>
                <br>

                <h3>Video Inpainting</h3>
                <p style="text-align:center;">
                    <image src="images/inpainting.png" height="300px"></image>
                </p>
                     &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     GT &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     NeRV &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                     D-NeRV &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
                <br>
            </div>
        </div>


        <hr style="max-width: 880px;">
        <div class="container" style="max-width: 880px;">
            <div class="row">
                <div class="col-md-12">
                    <h3>Citation</h3>
                        <code>
                        @inproceedings{he2023dnerv,<br>
                        &nbsp; title  = {Towards Scalable Neural Representation for Diverse Videos},<br>
                        &nbsp; author = {He, Bo and Yang, Xitong and Wang, Hanyu and Wu, Zuxuan and Chen, Hao and Huang, Shuaiyi and Ren, Yixuan and Lim, Ser-Nam and Shrivastava, Abhinav},<br>
                        &nbsp; booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                        &nbsp; year   = {2023},<br>
                    }</code></div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br> The website template was borrowed from <a href="https://bmild.github.io">Ben Mildenhall</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>