<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
 -->
  <title>Bo He's Personal Website</title>
  
  <meta name="author" content="Bo He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bo He (何博)</name>
              </p>
              <p>Bo He is a fifth year Ph.D. student in the department of Computer Science at University of Maryland, College Park, advised by <a href="https://www.cs.umd.edu/~abhinav/">Prof. Abhinav Shrivastava</a>. He obtained his Bachelor's degree at University of Chinese Academy of Sciences, China in 2018. During his undergraduate study, He was advised by <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ&hl=en">Prof. Xilin Chen</a> and <a href="https://scholar.google.com/citations?user=4AKCKKEAAAAJ&hl=en">Prof. Meina Kan</a> in the VIPL Group at the Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS).
              </p>

              <p style="text-align:center">
                Email: bohe [at] umd [dot] edu
              </p>
              <p style="text-align:center">
                <!-- <a href="mailto:bohe@umd.edu">Email</a> &nbsp/&nbsp -->
                <a href="images/Bo_He_Resume.pdf">CV</a> &nbsp/&nbsp 
                <a href="https://github.com/boheumd">Github</a> &nbsp/&nbsp 
                <a href="https://scholar.google.com/citations?user=G2RXa6EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/boheumd/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/bohe.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bohe_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tbody>
                  <tr>
                    <td>
                      <!-- <heading> -->
                        <!-- <font color="black">News</font> -->
                      <!-- </heading> -->
                      <p>
                      <ul>
                        <li>News: I am looking for an internship in 2023 Summer. If you find my research experiences match your position, feel free to contact me! </li>
                        <li>News (Sept 2022): One paper is accepted to BMVC 2022. </li>
                        <li>News (Jul 2022): One paper is accepted to ECCV 2022. </li>
                        <li>News (May 2022): I start an internship at Adobe working on multi-modal summarization task, supervised by Zhaowen Wang and Trung Bui. </li>
                        <li>News (Mar 2022): One paper is accepted to CVPR 2022.</li>
                      </ul>
                      </p>
                    </td>
                  </tr>
                </tbody>


            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in video-related tasks, especially on video understanding and video representation learning.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="imagination_stop()" onmouseover="imagination_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CNeRV_image'>
                  <img src='images/CNeRV.png' width="240"></div>
                <img src='images/CNeRV.png' width="240">
              </div>
              <script type="text/javascript">
                function imagination_start() {
                  document.getElementById('GNeRV_image').style.opacity = "1";
                }

                function imagination_stop() {
                  document.getElementById('GNeRV_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>

            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>CNeRV: Content-adaptive Neural Representation for Visual Data</papertitle>
              </a>
              <br>
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <a href="https://scholar.google.com/citations?user=lB9WkQ0AAAAJ&hl=en">Matt Gwilliam</a>,
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>BMVC</em>, 2022 (oral)  
              <br>
<!--               <a href="https://shuaiyihuang.github.io/publications/SCorrSAN/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.06974.pdf">arxiv</a> / 
              <a href="https://github.com/shuaiyihuang/SCorrSAN">code</a>   -->
              <p></p>
              <p>We propose neural visual representation with content-adaptive embedding, which combines the generalizability of autoencoders with the simplicity and compactness of implicit representation. We match the performance of NeRV, a state-of-the-art implicit neural representation, on the reconstruction task for frames seen during training while far surpassing for unseen frames that are skipped during training. </p>
            </td>


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='scsa_image'>
                  <img src='images/SCSA.png' width="240"></div>
                <img src='images/SCSA.png' width="240">
              </div>
              <script type="text/javascript">
                function sfp_start() {
                  document.getElementById('scsa_image').style.opacity = "1";
                }

                function sfp_stop() {
                  document.getElementById('scsa_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Learning Semantic Correspondence with Sparse Annotations</papertitle>
              </a>
              <br>
              <a href="https://shuaiyihuang.github.io/">Shuaiyi Huang</a>,
              <a href="https://www.loyo.me/">Luyu Yang</a>,
              <strong>Bo He</strong>,
              <a href="https://www.zhangsongyang.com/">Songyang Zhang</a>,
              <a href="https://xmhe.bitbucket.io/">Xuming He</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://shuaiyihuang.github.io/publications/SCorrSAN/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.06974.pdf">arxiv</a> / 
              <a href="https://github.com/shuaiyihuang/SCorrSAN">code</a>  

              <p></p>
              <p>We address the challenge of label sparsity in semantic correspondence by enriching supervision signals from sparse keypoint annotations. We first propose a teacher-student learning paradigm for generating dense pseudo-labels and then develop two novel strategies for denoising pseudo-labels. Our approach establishes the new state-of-the-art on three challenging benchmarks for semantic correspondence.</p>
            </td>

          </tr>  


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='asm_image'>
                  <img src='images/ASM-Loc.png' width="240"></div>
                <img src='images/ASM-Loc.png' width="240">
              </div>
              <script type="text/javascript">
                function sfp_start() {
                  document.getElementById('asm_image').style.opacity = "1";
                }

                function sfp_stop() {
                  document.getElementById('asm_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
              <a href="https://scholar.google.com/citations?user=ZTRGztgAAAAJ&hl=en">Le Kang</a>,
              <a href="https://scholar.google.com/citations?user=Jy6I9AIAAAAJ&hl=en">Zhiyu Cheng</a>,
              Xin Zhou,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
              <a href="https://boheumd.github.io/asm-loc">project page</a> / 
              <a href="https://arxiv.org/pdf/2203.15187v1.pdf">arxiv</a> / 
              <a href="https://github.com/boheumd/asm-loc">code</a>  

              <p></p>
              <p>We propose ASM-Loc, a novel weakly supervised temporal action localization framework that enables explicit, action-aware segment modeling beyond standard MIL-based methods. We establish new state of the art on THUMOS-14 and ActivityNet-v1.3 datasets.</p>
            </td>

          </tr>  


          <tr onmouseout="DVPvp_stop()" onmouseover="DVPvp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='NeRV_image'>
                  <img src='images/ActionStates.png' width="240"></div>
                <img src='images/ActionStates.png' width="240">
              </div>
              <script type="text/javascript">
                function DVPvp_start() {
                  document.getElementById('ActionStates_image').style.opacity = "1";
                }

                function DVPvp_stop() {
                  document.getElementById('ActionStates_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Recognizing Actions using Object States</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=VsTvk-8AAAAJ&hl=en">Nirat Saini</a>,
              <strong>Bo He</strong>,
              <a href="http://www.cs.umd.edu/~gauravsh/">Gaurav Shrivastava</a>,
              <a href="https://rssaketh.github.io/">Sai Saketh Rambhatla</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>ICLR Workshop</em>, 2022  
              <br>
              <a href="https://openreview.net/pdf?id=HBgzIHOUqgq">arxiv</a>
              <p></p>
              <p>We propose a computational framework that uses only two object states, start and end, and learns to recognize the underlying actions.</p>
            </td>
          </tr>  

          <tr onmouseout="DVPvp_stop()" onmouseover="DVPvp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='NeRV_image'>
                  <img src='images/NeRV.png' width="240"></div>
                <img src='images/NeRV.png' width="240">
              </div>
              <script type="text/javascript">
                function DVPvp_start() {
                  document.getElementById('NeRV_image').style.opacity = "1";
                }

                function DVPvp_stop() {
                  document.getElementById('NeRV_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>NeRV: Neural Representations for Videos</papertitle>
              </a>
              <br>
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <strong>Bo He</strong>,
              <a href="https://hywang66.github.io/">Hanyu Wang</a>,
              <a href="https://scholar.google.com/citations?user=TFpdak8AAAAJ&hl=en">Yixuan Ren</a>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>NeurIPS </em>, 2021  
              <br>
              <a href="https://haochen-rye.github.io/NeRV">project page</a> / 
              <a href="https://arxiv.org/pdf/2110.13903.pdf">arxiv</a> / 
              <a href="https://github.com/haochen-rye/nerv">code</a> 
              <p></p>
              <p>We propose a novel image-wise neural representation (NeRV) to encodes videos in neural networks, which takes frame index as input and outputs the corresponding RGB image. Compared to image-wise neural representation, NeRV imrpoves encoding speed by 25× to 70×, decoding speed by 38× to 132×. And it also shows comparable preformance for visual compression and denoising task.</p>
            </td>
          </tr>  

          <tr onmouseout="flash_stop()" onmouseover="flash_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='GTA_image'>
                  <img src='images/GTA.png' width="240"></div>
                <img src='images/GTA.png' width="240">
              </div>
              <script type="text/javascript">
                function flash_start() {
                  document.getElementById('GTA_image').style.opacity = "1";
                }

                function flash_stop() {
                  document.getElementById('GTA_image').style.opacity = "0";
                }
                videodepth_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>GTA: Global Temporal Attention for Video Action Understanding</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
              <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>BMVC </em>, 2021  
              <br>
              <a href="https://arxiv.org/pdf/2012.08510.pdf">arxiv</a>
              <p></p>
              <p>We introduce Global Temporal Attention (GTA), which performs global temporal attention on top of spatial attention in a decoupled manner. We apply GTA on both pixels and semantically similar regions to capture temporal relationships at different levels of spatial granularity.</p>
            </td>
          </tr>  
          

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
                <ul>
                  <li>Program Committee/Reviewers: CVPR, ICCV, ECCV, AAAI
                </ul>
            </td>
          </tr>
        </tbody></table>
        
<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
                <ul>
                  <li>RedBird PhD Scholarship, HKUST, 2021</li>
                  <li>National Scholarship, 2017</li>
                  <li>Outstanding Graduate (Zhejiang University), 2018</li>            
                  <li>First-Class Scholarship for Outstanding Merits, 2017</li>            
                  <li>Excellent Student Award, 2016, 2017</li>              
                </ul>              
            </td>
          </tr>
        </tbody></table> -->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
                <ul>
                  <li>CMSC216: Introduction to Computer Systems (Fall 2018)</li>
                </ul>
           </td>
          </tr>
        </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">Thank <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing the source code of his personal page.</p>
                <!-- <br> -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?bohe"
border="0" alt="Web Counters"></a>
<br><a href="https://www.easycounter.com/">Web Counters</a>

</html>
