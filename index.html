<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-MR953MJ4');</script>
  <!-- End Google Tag Manager -->

  <title>Bo He's Personal Website</title>
  
  <meta name="author" content="Bo He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MR953MJ4"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bo He (何博)</name>
              </p>
              <p>
                I am currently a research scientist at Meta. I have graduated from the department of Computer Science at University of Maryland, College Park, advised by <a href="https://www.cs.umd.edu/~abhinav/">Prof. Abhinav Shrivastava</a>. I obtained my Bachelor's degree at University of Chinese Academy of Sciences, China in 2018. 
              </p>
              <p>
                I'm interested in video-related tasks, especially on video understanding, video compression, and multimodal learning. 
                <!-- Recently, I start to explore how to apply large language models in video understanding tasks. -->
              </p>

              <p style="text-align:center">
                Email: bohe [at] umd [dot] edu
              </p>
              <p style="text-align:center">
                <a href="https://drive.google.com/file/d/1D0_FDOM7lBVvgNXRQryb3FfSAVYq-teK/view?usp=sharing">CV</a> &nbsp/&nbsp 
                <a href="https://github.com/boheumd">Github</a> &nbsp/&nbsp 
                <a href="https://scholar.google.com/citations?user=G2RXa6EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/boheumd/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/bohe.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bohe2_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tbody>
                  <tr>
                    <td>
                      <heading>
                        <font color="black">Updates</font>
                      </heading>
                      <p>
                      <ul>
                        <li>[<font color="red"><strong>New</strong></font>] I am seeking a full-time industry position beginning in early 2024. With my background in computer vision and video-related experiences, I am excited to apply my skills in new career opportunities. If you have any open positions that align with my expertise, please reach out.</li>
                      </ul>
                      </p>
                    </td>
                  </tr>
                </tbody>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tbody>
                  <tr>
                    <td>
                      <heading>
                        <font color="black">News</font>
                      </heading>
                      <p>
                      <ul>
                        <li><font color="#777777">[2024-03]</font> Two papers are accepted to CVPR 2024. </li>
                        <li><font color="#777777">[2024-02]</font> I joined Meta as a research scientist. </li>
                        <li><font color="#777777">[2023-07]</font> One paper is accepted to ICCV 2023. </li>
                        <li><font color="#777777">[2023-05]</font> I start an internship at Meta working on large language models in video understanding tasks. </li>
                        <li><font color="#777777">[2023-03]</font> Two papers are accepted to CVPR 2023. </li>
                        <!-- <li>[Nov 2022] : I am looking for an internship in 2023 Summer. If you find my research experiences match your position, feel free to contact me! </li> -->
                        <li><font color="#777777">[2022-09]</font> One paper is accepted to BMVC 2022. </li>
                        <li><font color="#777777">[2022-07]</font> One paper is accepted to ECCV 2022. </li>
                        <li><font color="#777777">[2022-05]</font> I start an internship at Adobe working on multi-modal summarization task, supervised by Zhaowen Wang and Trung Bui. </li>
                        <li><font color="#777777">[2022-03]</font> One paper is accepted to CVPR 2022. </li>
                      </ul>
                      </p>
                    </td>
                  </tr>
                </tbody>
        </tbody></table>


        <br><br><br>
        <heading>
          <font color="black">Publications</font>
        </heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/MA-LMM.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://henrylee2570.github.io/">Hengduo Li</a>,
              <a href="https://scholar.google.com/citations?user=7bujHzUAAAAJ&hl=en">Young Kyun Jang</a>,
              <a href="https://kmnp.github.io/">Menglin Jia</a>,
              <a href="https://scholar.google.com/citations?user=QntOfrwAAAAJ&hl=en">Xuefei Cao</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
              <a href="https://sites.google.com/site/sernam">Ser-Nam Lim</a>
              <br>
              <em>CVPR</em>, 2024  
              <br>
              <a href="https://boheumd.github.io/MA-LMM/">project page</a> / 
              <a href="https://arxiv.org/abs/2404.05726">arxiv</a> / 
              <a href="https://github.com/boheumd/MA-LMM">code</a>  
              <p></p>
              <p>We propose a memory-augmented large multimodal model for efficient and effective long-term video understanding ability. Our model can achieve state-of-the-art performances across multiple tasks such as long-video understanding, video question answering, and video captioning. </p>
            </td>
          </tr>  


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/OmniViD.jpg' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>OmniViD: A Generative Framework for Universal Video Understanding</papertitle>
              </a>
              <br>
              <a href="https://www.wangjunke.info/">Junke Wang</a>,
              <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>,
              <a href="https://scholar.google.com/citations?user=01iBf38AAAAJ">Chong Luo</a>,
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ">Lu Yuan</a>,
              <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
              <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ">Yu-Gang Jiang</a>
              <br>
              <em>CVPR</em>, 2024  
              <br>
              <a href="https://arxiv.org/abs/2403.17935">arxiv</a> / 
              <a href="https://github.com/wangjk666/OmniVid">code</a>  
              <p></p>
              <p>We seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. </p>
            </td>
          </tr>  


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/INSTRUCT4V.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</papertitle>
              </a>
              <br>
              <a href="https://www.wangjunke.info/">Junke Wang</a>,
              <a href="https://menglcool.github.io/">Lingchen Meng</a>,
              <a href="https://scholar.google.com/citations?user=qMT0sqAAAAAJ">Zejia Weng</a>,
              <strong>Bo He</strong>,
              <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
              <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ">Yu-Gang Jiang</a>
              <br>
              <em>arXiv</em> 
              <br>
              <a href="https://arxiv.org/abs/2311.07574">arxiv</a> / 
              <a href="https://github.com/X2FD/LVIS-INSTRUCT4V">code</a>  

              <p></p>
              <p>We introduce a fine-grained visual instruction dataset, LVIS-Instruct4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. Notably, by simply replacing the LLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA on most challenging LMM benchmarks.</p>
            </td>
          </tr>  


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/choplearn.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Chop & Learn: Recognizing and Generating Object-State Compositions</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=VsTvk-8AAAAJ&hl=en">Nirat Saini*</a>,
              <a href="https://hywang66.github.io/">Hanyu Wang*</a>,
              <a href="https://archana1998.github.io/">Archana Swaminathan</a>,
              <a href="https://vinojjayasundara.github.io/">Vinoj Jayasundara</a>,
              <a href="https://kampta.github.io/">Kamal Gupta</a>,
              <strong>Bo He</strong>,
              <br>
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>ICCV</em>, 2023  
              <br>
              <a href="https://chopnlearn.github.io/">project page</a> / 
              <a href="https://arxiv.org/abs/2309.14339">arxiv</a> / 
              <a href="https://drive.google.com/drive/folders/1QylDeUJ8h-CjLRJ8Z9bsdCoQ2uMs59W_">code</a>  

              <p></p>
              <p>We focus the task of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop & Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints.</p>
            </td>
          </tr>  

          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/D-NeRV.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Towards Scalable Neural Representation for Diverse Videos</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
              <a href="https://hywang66.github.io/">Hanyu Wang</a>,
              <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <a href="https://shuaiyihuang.github.io/">Shuaiyi Huang</a>,
              <br>
              <a href="https://scholar.google.com/citations?user=TFpdak8AAAAJ&hl=en">Yixuan Ren</a>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>CVPR</em>, 2023  
              <br>
              <a href="https://boheumd.github.io/D-NeRV">project page</a> / 
              <a href="https://arxiv.org/abs/2303.14124">arxiv</a> / 
              <a href="https://github.com/boheumd/D-NeRV">code</a>  

              <p></p>
              <p>We propose D-NeRV, a novel implicit neural representation based framework designed to encode large-scale and diverse videos. It achieves state-of-the-art performances on video compression.</p>
            </td>
          </tr>  

          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/A2Summ.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Align and Attend: Multimodal Summarization with Dual Contrastive Losses</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://junwang.umiacs.io/">Jun Wang</a>,
              <a href="https://www.cs.cmu.edu/~jielinq/">Jielin Qiu</a>,
              <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
              <a href="https://scholar.google.com/citations?user=lwlYARMAAAAJ&hl=en">Zhaowen Wang</a>
              <br>
              <em>CVPR</em>, 2023  
              <br>
              <a href="https://boheumd.github.io/A2Summ">project page</a> / 
              <a href="https://arxiv.org/abs/2303.07284">arxiv</a> / 
              <a href="https://github.com/boheumd/A2Summ">code</a>  

              <p></p>
              <p>We propose A2Summ, a novel supervised multimodal summarization framework that summarize video frames and text sentences with time correspondence. We also collect a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated summaries.</p>
            </td>

          </tr>  

          <tr onmouseout="imagination_stop()" onmouseover="imagination_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/CNeRV.png' width="240">
            </td>

            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>CNeRV: Content-adaptive Neural Representation for Visual Data</papertitle>
              </a>
              <br>
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <a href="https://scholar.google.com/citations?user=lB9WkQ0AAAAJ&hl=en">Matt Gwilliam</a>,
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>BMVC</em>, 2022 (oral)  
              <br>
              <a href="https://haochen-rye.github.io/CNeRV/">project page</a> / 
              <a href="https://arxiv.org/pdf/2211.10421.pdf">arxiv</a>
              <p></p>
              <p>We propose neural visual representation with content-adaptive embedding, which combines the generalizability of autoencoders with the simplicity and compactness of implicit representation. We match the performance of NeRV, a state-of-the-art implicit neural representation, on the reconstruction task for frames seen during training while far surpassing for unseen frames that are skipped during training. </p>
            </td>


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/SCSA.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Learning Semantic Correspondence with Sparse Annotations</papertitle>
              </a>
              <br>
              <a href="https://shuaiyihuang.github.io/">Shuaiyi Huang</a>,
              <a href="https://www.loyo.me/">Luyu Yang</a>,
              <strong>Bo He</strong>,
              <a href="https://www.zhangsongyang.com/">Songyang Zhang</a>,
              <a href="https://xmhe.bitbucket.io/">Xuming He</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://shuaiyihuang.github.io/publications/SCorrSAN/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.06974.pdf">arxiv</a> / 
              <a href="https://github.com/shuaiyihuang/SCorrSAN">code</a>  

              <p></p>
              <p>We address the challenge of label sparsity in semantic correspondence by enriching supervision signals from sparse keypoint annotations. We first propose a teacher-student learning paradigm for generating dense pseudo-labels and then develop two novel strategies for denoising pseudo-labels. Our approach establishes the new state-of-the-art on three challenging benchmarks for semantic correspondence.</p>
            </td>

          </tr>  


          <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/ASM-Loc.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
              <a href="https://scholar.google.com/citations?user=ZTRGztgAAAAJ&hl=en">Le Kang</a>,
              <a href="https://scholar.google.com/citations?user=Jy6I9AIAAAAJ&hl=en">Zhiyu Cheng</a>,
              Xin Zhou,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
              <a href="https://boheumd.github.io/ASM-Loc">project page</a> / 
              <a href="https://arxiv.org/pdf/2203.15187v1.pdf">arxiv</a> / 
              <a href="https://github.com/boheumd/ASM-Loc">code</a>  

              <p></p>
              <p>We propose ASM-Loc, a novel weakly supervised temporal action localization framework that enables explicit, action-aware segment modeling beyond standard MIL-based methods. We establish new state of the art on THUMOS-14 and ActivityNet-v1.3 datasets.</p>
            </td>

          </tr>  


          <tr onmouseout="DVPvp_stop()" onmouseover="DVPvp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/ActionStates.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Recognizing Actions using Object States</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=VsTvk-8AAAAJ&hl=en">Nirat Saini</a>,
              <strong>Bo He</strong>,
              <a href="http://www.cs.umd.edu/~gauravsh/">Gaurav Shrivastava</a>,
              <a href="https://rssaketh.github.io/">Sai Saketh Rambhatla</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>ICLR Workshop</em>, 2022  
              <br>
              <a href="https://openreview.net/pdf?id=HBgzIHOUqgq">arxiv</a>
              <p></p>
              <p>We propose a computational framework that uses only two object states, start and end, and learns to recognize the underlying actions.</p>
            </td>
          </tr>  

          <tr onmouseout="DVPvp_stop()" onmouseover="DVPvp_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/NeRV.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>NeRV: Neural Representations for Videos</papertitle>
              </a>
              <br>
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <strong>Bo He</strong>,
              <a href="https://hywang66.github.io/">Hanyu Wang</a>,
              <a href="https://scholar.google.com/citations?user=TFpdak8AAAAJ&hl=en">Yixuan Ren</a>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>NeurIPS </em>, 2021  
              <br>
              <a href="https://haochen-rye.github.io/NeRV">project page</a> / 
              <a href="https://arxiv.org/pdf/2110.13903.pdf">arxiv</a> / 
              <a href="https://github.com/haochen-rye/nerv">code</a> 
              <p></p>
              <p>We propose a novel image-wise neural representation (NeRV) to encodes videos in neural networks, which takes frame index as input and outputs the corresponding RGB image. Compared to image-wise neural representation, NeRV imrpoves encoding speed by 25× to 70×, decoding speed by 38× to 132×. And it also shows comparable preformance for visual compression and denoising task.</p>
            </td>
          </tr>  

          <tr onmouseout="flash_stop()" onmouseover="flash_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/GTA.png' width="240">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>GTA: Global Temporal Attention for Video Action Understanding</papertitle>
              </a>
              <br>
              <strong>Bo He</strong>,
              <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
              <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
              <a href="https://haochen-rye.github.io/">Hao Chen</a>,
              <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
              <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <br>
              <em>BMVC </em>, 2021  
              <br>
              <a href="https://arxiv.org/pdf/2012.08510.pdf">arxiv</a>
              <p></p>
              <p>We introduce Global Temporal Attention (GTA), which performs global temporal attention on top of spatial attention in a decoupled manner. We apply GTA on both pixels and semantically similar regions to capture temporal relationships at different levels of spatial granularity.</p>
            </td>
          </tr>  
          

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
                <ul>
                  <li>Program Committee/Reviewers: CVPR, ICCV, ECCV, AAAI, NeurIPS, TPAMI
                </ul>
            </td>
          </tr>
        </tbody></table>
        
<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
                <ul>
                  <li>RedBird PhD Scholarship, HKUST, 2021</li>
                  <li>National Scholarship, 2017</li>
                  <li>Outstanding Graduate (Zhejiang University), 2018</li>            
                  <li>First-Class Scholarship for Outstanding Merits, 2017</li>            
                  <li>Excellent Student Award, 2016, 2017</li>              
                </ul>              
            </td>
          </tr>
        </tbody></table> -->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
                <ul>
                  <li>CMSC216: Introduction to Computer Systems (Fall 2018)</li>
                </ul>
           </td>
          </tr>
        </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">Thank <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing the source code of his personal page.</p>
                <!-- <br> -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?bohe"
border="0" alt="Web Counters"></a>
<br><a href="https://www.easycounter.com/">Web Counters</a>

</html>
